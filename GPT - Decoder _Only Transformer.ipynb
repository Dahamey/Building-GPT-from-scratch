{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cc80132",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center;\">Building a GPT from scratch : Decoder-Only Transformer</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bc036a",
   "metadata": {},
   "source": [
    "\n",
    "<u>**Author</u> :** [Younes Dahami](https://www.linkedin.com/in/dahami/)\n",
    "\n",
    "# I - Introduction\n",
    "\n",
    "## 1) Tranformer\n",
    "\n",
    "The **Transformer** is a neural network architecture introduced in the [Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf) landmark research paper by Vaswani et al in 2017. It revolutionized the field of natural language processing (NLP) by offering a highly <u>**parallelizable**</u> and <u>**efficient**</u> way to process sequential data, such as text. The Transformer model is based on a mechanism called **self-attention**, which allows it to weigh the importance of different parts of the input sequence when generating an output sequence.\n",
    "\n",
    "At its core, the Transformer consists of an **encoder-decoder architecture**, with multiple layers of self-attention and feedforward neural networks. The encoder processes the input sequence to create contextualized representations, while the decoder generates the output sequence based on these representations. Each layer in the Transformer is designed to capture different levels of abstraction and dependencies within the input sequence.\n",
    "\n",
    "The key components of the Transformer include :\n",
    "\n",
    "* <u>**Positional Encoding**</u> **:** Since the Transformer architecture does not inherently understand the order of words in a sequence, positional encodings are added to the input embeddings to provide information about the position of each word.\n",
    "\n",
    "* <u>**Self-Attention Mechanism**</u> **:** This mechanism allows the model to weigh the importance of each word in the input sequence when generating each word in the output sequence. It computes **attention scores** between all pairs of words in the input sequence and uses them to generate a context vector for each word.\n",
    "\n",
    "* <u>**Multi-Head Attention**</u> **:** To capture different types of relationships within the input sequence, the Transformer employs multiple attention heads in parallel. Each head learns different attention patterns, enhancing the model's ability to capture diverse dependencies.\n",
    "\n",
    "* <u>**Feedforward Neural Networks**</u> **:** Each layer in the Transformer contains feedforward neural networks that process the output of the self-attention mechanism. These networks consist of fully connected layers followed by activation functions, allowing the model to capture complex patterns in the data.\n",
    "\n",
    "* <u>**Layer Normalization and Residual Connections (Add & Norm)**</u> **:** To stabilize training and facilitate the flow of gradients, layer normalization and residual connections are applied after each sublayer in the Transformer.\n",
    "\n",
    "\n",
    "<img src=\"TRANSFORMER.JPG\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "<center>Figure 1: The Transformer - model architecture</center>\n",
    "\n",
    "**Remark :** The encoder and the decoder both composed of a stack of $N$ identical layers\n",
    "\n",
    "The Transformer architecture has become the foundation for many state-of-the-art models in NLP, including **BERT**, **GPT**, and **T5**, among others. Its ability to capture long-range dependencies and its parallelizability make it well-suited for a wide range of sequence processing tasks, including machine translation, text generation, sentiment analysis, and more.\n",
    "\n",
    "## 2) State-of-the-Art NLP Architectures\n",
    "\n",
    "### a) BERT (Bidirectional Encoder Representations from Transformers)\n",
    "\n",
    "**BERT** is a transformer-based model introduced by Google in 2018 for natural language understanding tasks. It is pre-trained on large corpora of text using a masked language model (MLM) objective and next sentence prediction (NSP) task.\n",
    "Its key features include :\n",
    "\n",
    "* <u>**Bidirectionality**</u> **:** BERT is bidirectional, meaning it can understand the context of a word based on both its left and right context in a sentence.\n",
    "\n",
    "* <u>**Transformer Architecture**</u> **:** BERT utilizes the transformer architecture, consisting of encoder layers to learn contextual representations of words.\n",
    "\n",
    "* <u>**Pre-training Objectives**</u> **:** BERT is pre-trained using two main objectives: Masked Language Model (MLM) and Next Sentence Prediction (NSP). MLM involves masking random words in a sentence and predicting them based on the surrounding context. NSP involves predicting whether two sentences are consecutive in the original text.\n",
    "\n",
    "* <u>**Applications**</u> **:** BERT has been used for various NLP tasks, including text classification, named entity recognition, question answering, and sentiment analysis.\n",
    "\n",
    "### b) GPT (Generative Pre-trained Transformer)\n",
    "\n",
    "**GPT** is a series of transformer-based models developed by OpenAI for natural language generation tasks. The models are trained using a causal language modeling objective.\n",
    "Its key features include :\n",
    "\n",
    "* <u>**Unidirectionality**</u> **:** GPT is unidirectional, meaning it generates text in a left-to-right manner based only on the preceding context.\n",
    "\n",
    "* <u>**Transformer Architecture**</u> **:** Like BERT, GPT also utilizes the transformer architecture, but it focuses on decoding rather than encoding.\n",
    "\n",
    "* <u>**Causal Language Modeling**</u> **:** GPT is pre-trained using a causal language modeling objective, where the model predicts the next word in a sequence given the preceding context.\n",
    "\n",
    "* <u>**Applications**</u> **:** GPT has been used for tasks such as text generation, story writing, dialogue systems, and language translation.\n",
    "\n",
    "### c) T5 (Text-To-Text Transfer Transformer)\n",
    "\n",
    "**T5** is a transformer-based model introduced by Google in 2019. It is designed to unify different NLP tasks into a single text-to-text framework, where each task is treated as a text-to-text mapping problem.\n",
    "Its key features include :\n",
    "\n",
    "* <u>**Unified Framework**</u> **:** T5 treats all NLP tasks as text-to-text problems, where both input and output are represented as text strings.\n",
    "\n",
    "* <u>**Transformer Architecture**</u> **:** Similar to BERT and GPT, T5 utilizes the transformer architecture. However, it is trained to perform various tasks in a single model.\n",
    "\n",
    "* <u>**Pre-training & Fine-Tuning**</u> **:** T5 is pre-trained using a large corpus of text with a text-to-text objective. It can then be fine-tuned on specific downstream tasks by providing task-specific prompts.\n",
    "\n",
    "* <u>**Applications**</u> **:** T5 can be applied to a wide range of NLP tasks, including text summarization, machine translation, question answering, and language understanding.\n",
    "\n",
    "All these models represent significant advancements in natural language processing and have achieved state-of-the-art performance on various benchmarks and tasks in the field of NLP.\n",
    "\n",
    "## 3) Decoder-Only Transformer\n",
    "\n",
    "A **Decoder-Only Transformer** is a type of neural network architecture based on the Transformer model, commonly used in NLP tasks such as machine translation and text generation. Unlike the original Transformer architecture, which consists of both encoder and decoder components, a Decoder-Only Transformer only contains the decoder part.\n",
    "\n",
    "In a typical Transformer model, the encoder processes the input sequence and generates contextual representations, which are then passed to the decoder to generate the output sequence. However, in a Decoder-Only Transformer, <u>**there is no encoder**</u>, instead, the decoder directly generates the output sequence based on the target sequence or token. This means that the model doesn't have access to the source sequence during the decoding process, making it **suitable for tasks where only the target sequence is available during inference**, such as language generation tasks.\n",
    "\n",
    "Decoder-Only Transformers are often used in scenarios like autoregressive language modeling, where the model predicts the next token in a sequence given the preceding tokens. They are also employed in tasks like text summarization and dialogue generation, where the goal is to generate coherent and contextually relevant output based solely on the provided target information.\n",
    "\n",
    "<div style=\"display: flex;\">\n",
    "    <img src=\"encoder_decoder_transformer.png\" alt=\"Drawing\" style=\"width: 495px;\"/>\n",
    "     <img src=\"decoder_only_transformer.png\" alt=\"Drawing\" style=\"width: 495px;\"/>\n",
    "</div>\n",
    "<center>Figure 2: The Transformer model architecture for Encoder-Decoder (left) & Decoder-Only (right) </center>\n",
    "\n",
    "The two illustrations in Figure 2 are from [Statquest](https://www.statquest.org)\n",
    "\n",
    "#### Important papers : \n",
    "\n",
    "- [Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf) paper\n",
    "- [OpenAI GPT-3](https://arxiv.org/pdf/2005.14165.pdf) paper\n",
    "- [OpenAI ChatGPT](https://openai.com/blog/chatgpt) blog post"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0697649d",
   "metadata": {},
   "source": [
    "# II - Building the Decoder-Only Transformer from scratch\n",
    "\n",
    "## 1) Importing the data\n",
    "\n",
    "We always start with a dataset to train on. Let's download the tiny shakespeare dataset :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5e9c216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./input.txt', <http.client.HTTPMessage at 0x1956c2c1c00>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "urlretrieve(url, './input.txt') # create a text file named input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345d18bc",
   "metadata": {},
   "source": [
    "##  2) Reading from the file\n",
    "\n",
    "To read the contents of a file, we first need to open the file using the built-in function`open` function. The `open()` function returns a file object and provides several methods for interacting with the file's contents. The `open()` function also accepts a `mode` argument to specify how we can interact with the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a791d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mode = 'r' : open for reading\n",
    "# file.read() : to view the contents of the file\n",
    "\n",
    "with open('./input.txt','r', encoding = 'utf-8') as f :\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee758e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text contains 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "# length of the text (in terms of characters)\n",
    "print(f\"The text contains {len(text)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8b80128",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "Second Citizen:\n",
      "Would you proceed especially against Caius Marcius?\n",
      "\n",
      "All:\n",
      "Against him first: he's a very dog to the commonalty.\n",
      "\n",
      "Second Citizen:\n",
      "Consider you what services he has done for his country?\n",
      "\n",
      "First Citizen:\n",
      "Very well; and could be content to give him good\n",
      "report fort, but that he pays himself with being proud.\n",
      "\n",
      "Second Citizen:\n",
      "Nay, but speak not maliciously.\n",
      "\n",
      "First Citizen:\n",
      "I say unto you, what he hath done famously, he did\n",
      "it to that end: though soft-conscienced men can be\n",
      "content to \n"
     ]
    }
   ],
   "source": [
    "# Let's take look at the first printed 1500 characters in the text\n",
    "print(text[:1500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e757c2f7",
   "metadata": {},
   "source": [
    "## 3) Building the dataset\n",
    "\n",
    "First, let's create our vocabulary that we're going to work with :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7350ac2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "The total number of unique characters in the text : 65\n"
     ]
    }
   ],
   "source": [
    "# All the unique character that occur in the text\n",
    "chars = sorted(list(set(text)))  # vocabulary\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(''.join(chars))\n",
    "print(f\"The total number of unique characters in the text : {vocab_size}\")  # nb of tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2070f264",
   "metadata": {},
   "source": [
    "After creating our vacabulary, we have to build our encoder and decoder, where :\n",
    "\n",
    "The **Encoder** takes a string `s` of characters `c`, and outputs a list of integers, where each integer represent the character's index in the vocabulary. Meanwhile, the **Decoder** takes a list `l` of integers `i` and turn it into characters before joining them to output a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c65f73b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "\n",
    "# mapping from integers to characters\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# chracter-level tokenizer\n",
    "# encoder: takes a string turns it into a lis of integers\n",
    "encode = lambda s: [stoi[c] for c in s] \n",
    "# decoder: takes a list of integers turns it into a string\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) \n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2b22ab",
   "metadata": {},
   "source": [
    "Let's now encode the entire text dataset and store it into a `torch.Tensor` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb9938e8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56,  ..., 58, 53,  1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data = torch.tensor(encode(text), dtype = torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "\n",
    "# the 1500 characters we looked at earlier that will go the GPT look like this\n",
    "print(data[:1500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635ea65e",
   "metadata": {},
   "source": [
    "## 4) Splitting the dataset\n",
    "\n",
    "Let's now split up the data into **$90\\%$ training** and **$10\\%$ evaluation** sets :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "101beb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data))\n",
    "\n",
    "# Training data : includes the inputs & targets (ground truths)\n",
    "train_data = data[:n]  # 90%\n",
    "\n",
    "# Evaluation data : includes the inputs & targets (ground truths)\n",
    "val_data = data[n:]    # the rest : 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c42b3043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape : torch.Size([1003854])\n",
      "Validation data shape : torch.Size([111540])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training data shape : {train_data.shape}\")\n",
    "print(f\"Validation data shape : {val_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "69efcdba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# context length (the time dimension T in (B,T,C))\n",
    "block_size = 8\n",
    "train_data[:block_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f12182f",
   "metadata": {},
   "source": [
    "We're going to feed `block_size` characters (in this example : 8) to our transformer. In fact, each input example, has `block_size` examples in it :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ef70e8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the input : torch.Size([8])\n",
      "The shape of the target : torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "X = train_data[:block_size]     # the input to the transformer\n",
    "y = train_data[1:block_size+1]  # targets for each position (ground truths)\n",
    "\n",
    "print(f\"The shape of the input : {X.shape}\")\n",
    "print(f\"The shape of the target : {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "71f3c9ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target: 47\n",
      "when input is tensor([18, 47]) the target: 56\n",
      "when input is tensor([18, 47, 56]) the target: 57\n",
      "when input is tensor([18, 47, 56, 57]) the target: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
     ]
    }
   ],
   "source": [
    "for t in range(block_size) :\n",
    "    context = X[:t+1]    # X[0], X[1],..., X[t] (t+1 is excluded)\n",
    "    target = y[t]\n",
    "    \n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11a6e57",
   "metadata": {},
   "source": [
    "Let's now build a uitility function that splits our training/evaluation data into **inputs** and **targets** for our transformer model, and let's call it `get_batch` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f851e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[ 1, 61, 47, 50, 50,  1, 42, 43],\n",
      "        [43,  1, 58, 46, 43,  1, 50, 53],\n",
      "        [42, 63, 40, 47, 56, 42,  2,  0],\n",
      "        [46, 39, 58,  1, 39, 56, 58,  1]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[61, 47, 50, 50,  1, 42, 43, 57],\n",
      "        [ 1, 58, 46, 43,  1, 50, 53, 39],\n",
      "        [63, 40, 47, 56, 42,  2,  0, 19],\n",
      "        [39, 58,  1, 39, 56, 58,  1, 58]])\n",
      "-----------------\n",
      "when the input is [1] the target: 61\n",
      "when the input is [1, 61] the target: 47\n",
      "when the input is [1, 61, 47] the target: 50\n",
      "when the input is [1, 61, 47, 50] the target: 50\n",
      "when the input is [1, 61, 47, 50, 50] the target: 1\n",
      "when the input is [1, 61, 47, 50, 50, 1] the target: 42\n",
      "when the input is [1, 61, 47, 50, 50, 1, 42] the target: 43\n",
      "when the input is [1, 61, 47, 50, 50, 1, 42, 43] the target: 57\n",
      "when the input is [43] the target: 1\n",
      "when the input is [43, 1] the target: 58\n",
      "when the input is [43, 1, 58] the target: 46\n",
      "when the input is [43, 1, 58, 46] the target: 43\n",
      "when the input is [43, 1, 58, 46, 43] the target: 1\n",
      "when the input is [43, 1, 58, 46, 43, 1] the target: 50\n",
      "when the input is [43, 1, 58, 46, 43, 1, 50] the target: 53\n",
      "when the input is [43, 1, 58, 46, 43, 1, 50, 53] the target: 39\n",
      "when the input is [42] the target: 63\n",
      "when the input is [42, 63] the target: 40\n",
      "when the input is [42, 63, 40] the target: 47\n",
      "when the input is [42, 63, 40, 47] the target: 56\n",
      "when the input is [42, 63, 40, 47, 56] the target: 42\n",
      "when the input is [42, 63, 40, 47, 56, 42] the target: 2\n",
      "when the input is [42, 63, 40, 47, 56, 42, 2] the target: 0\n",
      "when the input is [42, 63, 40, 47, 56, 42, 2, 0] the target: 19\n",
      "when the input is [46] the target: 39\n",
      "when the input is [46, 39] the target: 58\n",
      "when the input is [46, 39, 58] the target: 1\n",
      "when the input is [46, 39, 58, 1] the target: 39\n",
      "when the input is [46, 39, 58, 1, 39] the target: 56\n",
      "when the input is [46, 39, 58, 1, 39, 56] the target: 58\n",
      "when the input is [46, 39, 58, 1, 39, 56, 58] the target: 1\n",
      "when the input is [46, 39, 58, 1, 39, 56, 58, 1] the target: 58\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1989)  # for reproducibility\n",
    "\n",
    "batch_size = 4           # B : how many independent sequences will we process in parallel?\n",
    "block_size = 8           # T : what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    \"\"\"given a split (which is 'train' or 'val') this function will generate\n",
    "       a small batch of data of inputs X and targets y\"\"\"\n",
    "    \n",
    "    \n",
    "    data = train_data if split == 'train' else val_data\n",
    "    \n",
    "    # sampling random locations in the dataset to pull chunks (of size batch_size) from, between 0 and 'len(data) - block_size'\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) # shape(batch_size) = B\n",
    "    \n",
    "    # Extract the rows & stack them to become (B,T)=(batch_size,block_size) 2D-tensors\n",
    "    X = torch.stack([data[i:i+block_size] for i in ix])     # we stack (vertically) the 'B' vectors of dimension T ---> shape(B,T)\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # because each example has multiple examples in it; shape(B,T)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "\n",
    "#---------------------------------------------------\n",
    "\n",
    "Xb, yb = get_batch('train')  # b for batch\n",
    "print('inputs:')\n",
    "print(Xb.shape)\n",
    "print(Xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('-----------------')\n",
    "\n",
    "for b in range(batch_size): # across the batch dimension (row dim) B\n",
    "    for t in range(block_size): # across the time dimension (col dim) T\n",
    "        context = Xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when the input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc51b11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1, 61, 47, 50, 50,  1, 42, 43],\n",
      "        [43,  1, 58, 46, 43,  1, 50, 53],\n",
      "        [42, 63, 40, 47, 56, 42,  2,  0],\n",
      "        [46, 39, 58,  1, 39, 56, 58,  1]])\n"
     ]
    }
   ],
   "source": [
    "# Our batch of input to the transformer\n",
    "print(xb) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a2712c",
   "metadata": {},
   "source": [
    "Th transformer will process `batch_size` (in this example 4) strings of `block_size` (in this example 8) characters simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001f7189",
   "metadata": {},
   "source": [
    "## 5) Building the model\n",
    "\n",
    "In this section, we're going to construct our model and 'Pytorchify' the code, which involves making the code more similar to the conventions found in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d882fa",
   "metadata": {},
   "source": [
    "3D-Tensor's dimensions :\n",
    "\n",
    "* **B :** represents the batch dimension, indicating the number of samples in each batch.\n",
    "* **T :** represents the time dimension or sequence length, indicating the number of tokens or words in each sequence.\n",
    "* **C :** represents the channels dimension, often used to represent the embedding dimension or the vocabulary size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "155266bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of logits after forward is applied : torch.Size([32, 65])\n",
      "tensor(4.8965, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "Fxl3a!s.WkwD-Nb,ISnTXZYHx$O&VE',j'MlQwamcIw$YlyoKagTW\n",
      "\n",
      "\n",
      "$OBz!Pcn OVH?v?V$wDlpQBrCxHQODEBXsWvh'-aoNaz\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1989)\n",
    "\n",
    "# B : Batch dimension (batch_size),\n",
    "# T : Time dimension (block_size =  context_size),\n",
    "# C : Channels dimension (vocab_size) embedding dimension\n",
    "# REMARK : in this project we're working with (B,T,C) 3D-tensors, but in Pytorch & Tensorflow they work with (B,C,T) 3D-tensors\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size) :\n",
    "        super().__init__()  # inherets additional attributes from nn.Module\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) #(vocab_size, C)\n",
    "\n",
    "    def forward(self, idx, targets = None) :\n",
    "        # 'idx' (inputs) and 'targets' are both (B,T) shaped 2D-tensors of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T) -----> (B,T,C)\n",
    "        if targets is None :\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)  # reshaping the logits : (B,T,C)---> (B*T,C) that's how pytorch's crossentropy treats the logits dim in crossentropy \n",
    "            targets = targets.view(B*T)   # reshaping the targets : (B,T)----> B*T\n",
    "            loss = F.cross_entropy(logits, targets)  # loss\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \"\"\" 'idx' is (B, T)-array of indices in the current context in some batch.\n",
    "        The job of generate is basically to take (B,T) and extend it to be (B,T+1), (B,T+2)....\n",
    "        it basically continues the generation in all the Batch dimensions in the Time dimension. It will do\n",
    "        that for  'max_new_tokens' \"\"\"\n",
    "        \n",
    "        for _ in range(max_new_tokens) :\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)  # the forward method is automatically applied\n",
    "            # focus only on the last time step (hence -1)\n",
    "            logits = logits[:, -1, :] # (B,T,C)--->(B, C) \n",
    "            # apply softmax on the last dimension (C, hence -1) to get probabilities\n",
    "            probs = F.softmax(logits, dim = -1) # (B, C)=(batch_size, vocab_size)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples = 1) # (B, C)-->(B, 1) because from each row we sample one outcome (the next index)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim = 1) # (B, T) & (B,1)----concatenation--->(B, T+1)...--->(B, T+max_new_tokens)\n",
    "        return idx\n",
    "\n",
    "    \n",
    "    \n",
    "#-----------------------------------------------------------\n",
    "\n",
    "# Calculate the loss before training    \n",
    "m = BigramLanguageModel(vocab_size)   # '__init__' is automatically applied : an embedding table (65,65) is created\n",
    "logits, loss = m(Xb, yb)              # 'forward' is automatically applied\n",
    "print(f\"Shape of logits after forward is applied : {logits.shape}\")\n",
    "print(loss)\n",
    "\n",
    "\n",
    "# Generate 100 tokens without training the model\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype = torch.long), max_new_tokens = 100)[0].tolist()))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a2c75e",
   "metadata": {},
   "source": [
    "`idx = torch.zeros((1, 1),dtype=torch.long)` creates a torch tensor of shape(1,1) of type integers, with 0 in it, to kick off the generation. We index by `[0]` to unplack the single batch (because we worked with 1 batch) dimension that exists, and we convert it to a list to feed it to `decode`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d2288c",
   "metadata": {},
   "source": [
    "# Remark : Why do we sample from a multinomial distribution  ?\n",
    "\n",
    "Using the maximum probability token at each step during generation, also known as <u>**greedy decoding**</u>, can lead to suboptimal results in certain scenarios. Here's why sampling from the multinomial distribution is preferred over greedy decoding :\n",
    "\n",
    "* **Diversity :** Greedy decoding tends to produce repetitive or deterministic outputs because it always selects the token with the highest probability. This can result in generated sequences lacking diversity and variety. Sampling from the multinomial distribution allows for randomness in token selection, introducing diversity in the generated outputs.\n",
    "\n",
    "* **Exploration :** Sampling encourages the model to explore different possibilities in the output space. By considering tokens with lower probabilities, the model has the opportunity to generate alternative sequences that may not be immediately apparent based on the highest probability tokens alone. This exploration can lead to more creative and diverse outputs.\n",
    "\n",
    "* **Handling Uncertainty :** In some cases, the model may have high uncertainty about the next token to generate, with multiple tokens having relatively high probabilities. Sampling allows the model to express this uncertainty by considering multiple tokens with non-negligible probabilities, rather than committing to a single token deterministically.\n",
    "\n",
    "* **Avoiding Mode Collapse :** Greedy decoding is susceptible to mode collapse, where the model repeatedly generates similar or identical sequences. Sampling helps mitigate this issue by introducing randomness into the generation process, preventing the model from getting stuck in repetitive patterns.\n",
    "\n",
    "Overall, sampling from the multinomial distribution provides a more flexible and exploratory approach to sequence generation, allowing the model to produce diverse and varied outputs while handling uncertainty and avoiding mode collapse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d972b70",
   "metadata": {},
   "source": [
    "## 6) Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fa3018be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0319fdbc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.752437114715576\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "for steps in range(1000): # increase number of steps for good results...\n",
    "\n",
    "    # sample a batch of data\n",
    "    Xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    \n",
    "    # zeroing out all the gradients from the previous step\n",
    "    optimizer.zero_grad(set_to_none = True)\n",
    "    \n",
    "    # getting the gradients for all the parameters\n",
    "    loss.backward()\n",
    "    \n",
    "    # using those gradients to update our parameters\n",
    "    optimizer.step()\n",
    "\n",
    "# Print the final loss\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c46c17",
   "metadata": {},
   "source": [
    "Let's generate 500 tokens after this first training :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fd932cbc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ILZ;o!xgi,IfneEUBD-cFie F;Dcms,Ldnt EJMeey,rr  Bd AxUMfnCAZsGIiodmm:\n",
      "liSJ,?VLnKd !'tGq$JD\n",
      "WJZLo li.:\n",
      "WPro w,  rET-E'SNaks-QJWnNDXDI'vZQu!kj.j\n",
      "tdj-3;w a\n",
      "GqOHe?!P3Apsn3' M  lJ.p3C, E's.fn?HQKWAGSMcc. o,LjHdjUlf,QSIQqODGUlfsixsTOUI;a\n",
      "F&l:nIQUTAk\n",
      "ICcaU iedlYieexw ppl3:\n",
      "xY;u:SVcXqyTC!LIld.klI.yv?yv!f ruks A&Oc\n",
      "re$H&DWjHDORN'ilRcowpecBJ-GfAh:uQBx\n",
      "o olYI$::C!seion3cfGOIdTGRlt WESrtieclPyHCO'KOIwliULX3Fqnggph'YBpyNPFRM\n",
      "lolr?jChmep$.aAksid TInp3iGil,IGI'grV? ClyoPtC!&jM?jBvIZaVGIt.SIdSe w \n",
      "by, t e?iNuOyp\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c76570",
   "metadata": {},
   "source": [
    "### a) The Mathematical Trick in Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7118e751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.8097, -0.1067],\n",
      "         [-0.4749, -0.4006],\n",
      "         [-1.4039, -0.1104],\n",
      "         [ 0.7262, -0.4025],\n",
      "         [ 1.4481,  0.9492],\n",
      "         [ 1.0304, -0.3663],\n",
      "         [-0.3690,  0.6537],\n",
      "         [ 0.9952,  1.4014]],\n",
      "\n",
      "        [[ 0.1133, -2.4801],\n",
      "         [ 1.0788,  0.8122],\n",
      "         [-1.0402,  0.0253],\n",
      "         [-0.2241, -0.4291],\n",
      "         [-0.2994,  0.1843],\n",
      "         [ 0.1031, -0.6406],\n",
      "         [-0.0708,  0.3859],\n",
      "         [ 0.4271, -1.3242]],\n",
      "\n",
      "        [[-0.3533, -0.8130],\n",
      "         [ 0.4300, -0.0615],\n",
      "         [ 1.2262, -0.5112],\n",
      "         [ 0.6099,  0.1949],\n",
      "         [ 0.1370,  0.5844],\n",
      "         [ 0.1822,  0.0621],\n",
      "         [-0.7569, -0.5141],\n",
      "         [-0.7620, -1.7349]],\n",
      "\n",
      "        [[ 0.2587, -2.2229],\n",
      "         [ 0.9285,  0.0336],\n",
      "         [-0.8256,  0.9959],\n",
      "         [ 0.0420, -0.7232],\n",
      "         [ 1.0993, -0.5001],\n",
      "         [ 1.0548, -1.4340],\n",
      "         [-0.3470, -0.5874],\n",
      "         [-0.1756, -1.8884]]])\n",
      "torch.Size([4, 8, 2])\n"
     ]
    }
   ],
   "source": [
    "# consider the following toy example:\n",
    "\n",
    "torch.manual_seed(1989)\n",
    "B,T,C = 4,8,2 # batch, time (context), channels (embedding_size)\n",
    "X = torch.randn(B,T,C)   # 3D-tensor filled with random numbers from a standard normal distribution\n",
    "\n",
    "print(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6220a5db",
   "metadata": {},
   "source": [
    "These $8$ tokens are not \"talking\" to each other, but we want them to talk to each other, in such a way that token $t$ only communicates with with previous tokens and itself, and it should not communicate with tokens after it, because those are future tokens in the sequence, **no information come from the future because we're about to predict the future.** (masked attention for decoder-only transformer)\n",
    "\n",
    "   <img src=\"self_attention.png\" alt=\"Drawing\" style=\"width: 495px;\"/>\n",
    "   <img src=\"masked_self_attention.png\" alt=\"Drawing\" style=\"width: 495px;\"/>\n",
    "\n",
    "The easiest way for tokens to communicate, is calculating the average. So for example, if I'm the $5th$ token $t_5$, I would like to take the channels that are information at my step (the $5th$), but also the channels from the $4th$ step, $3rd$...$1st$ step, meaning ($t_4$, $t_3$, $t_2$, $t_1$), and I would like to average those up, and that would become sort of like a **feature vector that summarizes me in the context of my history.** \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<u>**ALERT :**</u> Calculating the average is actually extremely weak and lossy form of interaction, where we lose a ton of information about the spatial arrangement of all those tokens, but that's ok for now, we'll see how to bring that information back later.\n",
    "\n",
    "For now, what we would like to do is : For every single batch element independently, for every $t-th$ token in that sequence, we'd like to calculate the average of all the vectors in all the previous tokens and the current token ($i\\leq t$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "829ea2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want X[b,t] = mean_{i<=t} x[b,i] \n",
    "# bow : bag of words\n",
    "X_bow = torch.zeros((B,T,C))  # shape(B,T,C)\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        X_prev = X[b,:t+1] # shape(t,C)\n",
    "        X_bow[b,t] = torch.mean(X_prev, 0)   # The average accross the T dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "804932f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8097, -0.1067],\n",
       "        [-0.4749, -0.4006],\n",
       "        [-1.4039, -0.1104],\n",
       "        [ 0.7262, -0.4025],\n",
       "        [ 1.4481,  0.9492],\n",
       "        [ 1.0304, -0.3663],\n",
       "        [-0.3690,  0.6537],\n",
       "        [ 0.9952,  1.4014]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first element of the batch \n",
    "X[0]  #(T,C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "10478651",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8097, -0.1067],\n",
       "        [ 0.1674, -0.2536],\n",
       "        [-0.3564, -0.2059],\n",
       "        [-0.0857, -0.2550],\n",
       "        [ 0.2211, -0.0142],\n",
       "        [ 0.3559, -0.0729],\n",
       "        [ 0.2524,  0.0309],\n",
       "        [ 0.3452,  0.2022]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bow[0]          # averging the x[0] across the T dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa413d46",
   "metadata": {},
   "source": [
    "Each $i-th$ row from `xbow[0]` is the average of the rows $0$,$1$,...,$i$ from `x[0]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b159907",
   "metadata": {},
   "source": [
    "## b) Calculating The Average Using Matrix Multiplication For a Weighted Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6fa3cb98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# triangular lower portion\n",
    "torch.tril(torch.ones(3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "04ccac49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = \n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "-------\n",
      "b = \n",
      "tensor([[2., 6.],\n",
      "        [8., 5.],\n",
      "        [7., 8.]])\n",
      "-------\n",
      "c = \n",
      "tensor([[2.0000, 6.0000],\n",
      "        [5.0000, 5.5000],\n",
      "        [5.6667, 6.3333]])\n"
     ]
    }
   ],
   "source": [
    "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
    "torch.manual_seed(25)\n",
    "\n",
    "# these 2 lines of a, will help us calculate the averages \n",
    "a = torch.tril(torch.ones(3, 3))    # triangular lower portion : shape(3, 3)\n",
    "a = a / torch.sum(a, 1, keepdim = True)  # shape(3, 3) : every row of 'a' sums to 1\n",
    "\n",
    "b = torch.randint(0,10,(3,2)).float()  # shape(3,2)\n",
    "c = a @ b  # shape(3, 3) @ shape(3,2) ---> shape(3,2)\n",
    "\n",
    "print(\"a = \")\n",
    "print(a)\n",
    "print(\"-------\")\n",
    "print(\"b = \")\n",
    "print(b)\n",
    "print(\"-------\")\n",
    "print(\"c = \")\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f2c6f7",
   "metadata": {},
   "source": [
    "We use the triangular matrix so that each $i-th$ row from `c`, is the average of the rows $0,1,...i$ from `b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "06f8bc9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril(torch.ones(T, T))  # shape(8,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "40185d11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 2 : using matrix multiply for a weighted aggregation\n",
    "# wei short for weights\n",
    "\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)   # shape(T, T) : tril and every row sums up to 1\n",
    "X_bow2 = wei @ X                       # (T, T) @ (B, T, C)-----Pytorch----->(B, T, T) @ (B, T, C) ----> (B, T, C)\n",
    "torch.allclose(X_bow, X_bow2)          # True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e117f8e",
   "metadata": {},
   "source": [
    "<u>**Explanation of (B, T, T) @ (B, T, C) ----> (B, T, C)**</u> **:**\n",
    "\n",
    "In reality, it's **(T, T) @ (B, T, C)**, but Pytorch will come and see that these shapes are not the same, so it will create a batch dimension in **(T, T)**, the mulitplication becomes **(B, T, T) @ (B, T, C)**. And `@` is a **batched matrix multiply**, and so it will apply this matrix multiplication in all the batch elements in **parallel** and **individually**. And then for each batch element there will be a **(T, T) @ (T, C)**, exactly what we had above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040a0c4d",
   "metadata": {},
   "source": [
    "## c) Using Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e5c254e1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weight matrix : tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "The weight matrix : tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n",
      "torch.Size([4, 8, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 3: use Softmax\n",
    "tril = torch.tril(torch.ones(T, T))   # shape(T,T)\n",
    "wei = torch.zeros((T,T))              # shape(T,T)\n",
    "print(f\"The weight matrix : {wei}\")\n",
    "\n",
    "# all the 'tril' elements which are equal to zero, make them equal to '-inf'\n",
    "wei = wei.masked_fill(tril == 0, float(\"-inf\"))   # shape(T,T)\n",
    "\n",
    "# exponentiate every element of 'wei' and divide by their sum (the future cannot communicate with the past)\n",
    "wei = F.softmax(wei, dim = -1)  # dim = -1 refers to the last dimension of 'wei'\n",
    "print(f\"The weight matrix : {wei}\")\n",
    "\n",
    "X_bow3 = wei @ X  # (T,T)@(B,T,C)--->(B,T,T)@(B,T,C)--->(B,T,C)\n",
    "print(X_bow3.shape)\n",
    "torch.allclose(X_bow, X_bow3)  # True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d2149c",
   "metadata": {},
   "source": [
    "## 7) Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1fe92b7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 4: self-attention!\n",
    "torch.manual_seed(1989)\n",
    "\n",
    "# batch, time, channels : we have one batch of B=4 examples, each example is composed of T=8 tokens, each tokens is represented in C-dim =32-dim embedding\n",
    "B,T,C = 4,8,32 \n",
    "X = torch.randn(B,T,C)  #(B,T,C)\n",
    "\n",
    "# let's see a single Head perform self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias = False) # dim_input=C, and dim_output=head_size\n",
    "query = nn.Linear(C, head_size, bias = False) # bias=False means we're just doing a matrix multiplication\n",
    "value = nn.Linear(C, head_size, bias = False)\n",
    "\n",
    "# all the tokens in all the positions in (B,T) arrangements will produce (in parallel and independetly) a key and a query\n",
    "# no communication has happened yet\n",
    "k = key(X)   # 'X' : (B,T,32)----linear projection of 'X'---> 'k' : (B, T, 16)\n",
    "q = query(X) # 'X' : (B,T,32)----linear projection of 'X'---> 'q' : (B, T, 16)\n",
    "\n",
    "# communication will happen now, we transpose 'k' in the 2nd and 3rd dimensions\n",
    "# we're applying '.T' so that the dimensiosn of 'q' and 'k' allign\n",
    "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "\n",
    "# to avoid the communication with the future : Masked self-attention\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "#wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "\n",
    "# Calculate similiraty scores (also called 'weights')\n",
    "wei = F.softmax(wei, dim = -1)  # (T,T) : each row sums up to 1\n",
    "\n",
    "# Calculate attention scores\n",
    "v = value(X)    # 'X' : (B,T,32)----linear projection of 'X'---> 'k' : (B, T, 16)\n",
    "out = wei @ v   # (T,T)@(B, T, 16)-->(B,T,T)@(B,T,16)------->(B,T,16)\n",
    "#out = wei @ X\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "07ae30c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4555, 0.5445, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4064, 0.2620, 0.3317, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1848, 0.1477, 0.1866, 0.4810, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1926, 0.4573, 0.1975, 0.1488, 0.0039, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2538, 0.1568, 0.3911, 0.1029, 0.0299, 0.0654, 0.0000, 0.0000],\n",
       "        [0.0783, 0.0362, 0.4167, 0.0047, 0.0040, 0.0586, 0.4015, 0.0000],\n",
       "        [0.0714, 0.0753, 0.2513, 0.0920, 0.0418, 0.2746, 0.0480, 0.1456]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Represention of the similarity scores for the first example in the batch\n",
    "wei[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5de2b76",
   "metadata": {},
   "source": [
    "<u>**NOTES**</u> **:**\n",
    "- Attention is a **communication mechanism**, it can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "\n",
    "- There is no notion of space, Attention simply acts over a set of vectors. **This is why we need to positionally encode tokens before.**\n",
    "\n",
    "- Each example across batch dimension is, of course, processed completely **independently** and never \"talk\" to each other.\n",
    "\n",
    "- For an **encoder-attention block**, we just delete the single line that does the masking with `tril`, allowing all tokens to communicate. This block that we saw here is called a **decoder-attention block** because it has **triangular masking**, and is usually used in autoregressive settings, like language modeling.\n",
    "\n",
    "- **Self-attention** just means that the \"keys\" and \"values\" are produced from the same source as \"queries\". In **cross-attention**, the queries still get produced from `X`, but the \"keys\" and \"values\" come from some other, external source (e.g. an encoder module).\n",
    "\n",
    "- **Scaled attention additional** divides `wei` by $1/\\sqrt(HeadSize)$. This makes it so when input $Q, K$ are unit variance ($Var(Q)=Var(K)=1$), `wei` will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3a66cda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling 'k' and 'q' from standard normal distribution\n",
    "k = torch.randn(B,T,head_size) \n",
    "q = torch.randn(B,T,head_size)\n",
    "\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1d01494b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.9632), tensor(1.0314), tensor(1.0345))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.var(), q.var(), wei.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaf8a85",
   "metadata": {},
   "source": [
    "<u>**Remark**</u> **:** When the variance diverges from $1$ the softmax gets too peaky, it converges to **one-hot** as seen in the example below :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "43f56239",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6099304c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim = -1)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e8a1062c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])\n",
      "tensor([1.5851e-02, 7.8918e-04, 1.1713e-01, 7.8918e-04, 8.6545e-01])\n",
      "tensor([2.3555e-03, 2.6167e-05, 4.7312e-02, 2.6167e-05, 9.5028e-01])\n",
      "tensor([4.2484e-18, 3.9754e-31, 2.0612e-09, 3.9754e-31, 1.0000e+00])\n"
     ]
    }
   ],
   "source": [
    "print(torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim = -1))\n",
    "print(torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*10, dim = -1))\n",
    "print(torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*15, dim = -1))\n",
    "print(torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*100, dim = -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42fb03e",
   "metadata": {},
   "source": [
    "## 8) Normalization \n",
    "\n",
    "The statement :\n",
    "\n",
    "<center> \"The normalization layer normalizes the output of the attention mechanisms to stabilize training\"</center>\n",
    "\n",
    "is partly correct. Normalization layers, such as layer normalization or batch normalization, are indeed used in transformer architectures to stabilize training. However, their primary purpose is not specifically to normalize the output of the attention mechanisms. Instead, **Normalization layers are typically applied after each sub-layer within the transformer block to stabilize the gradients and improve the training dynamics of the entire network.**\n",
    "\n",
    "In the context of transformer models, normalization layers are commonly inserted after the \"self-attention mechanism\" and the \"feed-forward neural network (FFNN)\" sub-layers. These layers help mitigate issues such as the vanishing or exploding gradient problem and facilitate smoother optimization during training.\n",
    "\n",
    "So, while normalization layers play a crucial role in stabilizing training in transformer architectures, their function is broader than just normalizing the output of attention mechanisms. They contribute to the overall stability and efficiency of the training process for the entire model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2ac8184f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LayerNorm1d :\n",
    "    # (used to be BatchNorm1d) identical to layernorm in Pytorch\n",
    "    # Nomalizing rows instead of columns (like in the \"BatchNorm1d\" class from my other project \"Building makemore (III) : Activations & Gradients, BatchNorm)\"\"\n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1) :  \n",
    "        self.eps = eps                  # epsilon term in the normalization\n",
    "        self.gamma = torch.ones(dim)    # gain\n",
    "        self.beta = torch.zeros(dim)    # bias\n",
    "\n",
    "    def __call__(self, X) :\n",
    "        # calculate the forward pass\n",
    "        X_mean = X.mean(1, keepdim=True) # batch mean ; shape(X) = (B,T,C)\n",
    "        X_var = X.var(1, keepdim=True) # batch variance ; shape(X)\n",
    "        X_hat = (X - X_mean) / torch.sqrt(X_var + self.eps) # normalize to unit variance (B,T,C)\n",
    "        self.out = self.gamma * X_hat + self.beta # same shape(X) = (B,T,C); '*' is a element-wise multiplication\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self) :\n",
    "        return [self.gamma, self.beta]  # trainable parameters\n",
    "\n",
    "torch.manual_seed(1989)\n",
    "module = LayerNorm1d(100)\n",
    "X = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
    "X = module(X)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "de2f1333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.2395), tensor(0.8673))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:,0].mean(), X[:,0].std() # mean,std of one feature across all batch inputs, (not normalized) (1st column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "59a2be43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-3.8147e-08), tensor(1.0000))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0,:].mean(), X[0,:].std() # mean,std of a single input from the batch, of its features, (normalized) (1st row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "39b33c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# French to English translation example:\n",
    "# Encoder-decoder tranformer\n",
    "# <--------- ENCODE ------------------><--------------- DECODE ----------------->\n",
    "# J'ai appris beaucoup sur l'apprentissage profond. <START> I have learned a lot about deep learning.<END>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7887e32",
   "metadata": {},
   "source": [
    "# III - Complete code for reference - Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3e98df88",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.209729 Million parameters\n",
      "step 0: train loss 4.3353, val loss 4.3393\n",
      "step 100: train loss 2.6348, val loss 2.6639\n",
      "step 200: train loss 2.4876, val loss 2.5021\n",
      "step 300: train loss 2.4100, val loss 2.4162\n",
      "step 400: train loss 2.3420, val loss 2.3552\n",
      "step 500: train loss 2.2986, val loss 2.3093\n",
      "step 600: train loss 2.2304, val loss 2.2477\n",
      "step 700: train loss 2.2145, val loss 2.2200\n",
      "step 800: train loss 2.1520, val loss 2.1782\n",
      "step 900: train loss 2.1147, val loss 2.1541\n",
      "step 1000: train loss 2.0966, val loss 2.1397\n",
      "step 1100: train loss 2.0639, val loss 2.1016\n",
      "step 1200: train loss 2.0345, val loss 2.0703\n",
      "step 1300: train loss 2.0152, val loss 2.0661\n",
      "step 1400: train loss 1.9808, val loss 2.0434\n",
      "step 1500: train loss 1.9626, val loss 2.0361\n",
      "step 1600: train loss 1.9470, val loss 2.0334\n",
      "step 1700: train loss 1.9271, val loss 2.0159\n",
      "step 1800: train loss 1.8981, val loss 2.0030\n",
      "step 1900: train loss 1.8987, val loss 1.9885\n",
      "step 2000: train loss 1.8791, val loss 1.9884\n",
      "step 2100: train loss 1.8627, val loss 1.9745\n",
      "step 2200: train loss 1.8509, val loss 1.9586\n",
      "step 2300: train loss 1.8278, val loss 1.9525\n",
      "step 2400: train loss 1.8290, val loss 1.9490\n",
      "step 2500: train loss 1.8112, val loss 1.9405\n",
      "step 2600: train loss 1.8004, val loss 1.9169\n",
      "step 2700: train loss 1.7896, val loss 1.9157\n",
      "step 2800: train loss 1.7846, val loss 1.9147\n",
      "step 2900: train loss 1.7601, val loss 1.8915\n",
      "step 3000: train loss 1.7601, val loss 1.8947\n",
      "step 3100: train loss 1.7564, val loss 1.8896\n",
      "step 3200: train loss 1.7445, val loss 1.8935\n",
      "step 3300: train loss 1.7303, val loss 1.8750\n",
      "step 3400: train loss 1.7331, val loss 1.8838\n",
      "step 3500: train loss 1.7249, val loss 1.8631\n",
      "step 3600: train loss 1.7264, val loss 1.8666\n",
      "step 3700: train loss 1.7215, val loss 1.8466\n",
      "step 3800: train loss 1.6888, val loss 1.8398\n",
      "step 3900: train loss 1.6975, val loss 1.8587\n",
      "step 4000: train loss 1.6891, val loss 1.8467\n",
      "step 4100: train loss 1.7014, val loss 1.8431\n",
      "step 4200: train loss 1.6763, val loss 1.8264\n",
      "step 4300: train loss 1.6832, val loss 1.8312\n",
      "step 4400: train loss 1.6814, val loss 1.8381\n",
      "step 4500: train loss 1.6821, val loss 1.8215\n",
      "step 4600: train loss 1.6668, val loss 1.8050\n",
      "step 4700: train loss 1.6654, val loss 1.8200\n",
      "step 4800: train loss 1.6578, val loss 1.8068\n",
      "step 4900: train loss 1.6585, val loss 1.8012\n",
      "step 4999: train loss 1.6554, val loss 1.8139\n",
      "\n",
      "But frour with bowl, which me beg roubbe?\n",
      "\n",
      "SICINCENTER:\n",
      "So, think our hear triture,\n",
      "We dayforth, be cout in yied, the kefored,\n",
      "As newards and given all mine ling prums:\n",
      "I ward I preatious agonel'd I doum.\n",
      "\n",
      "QUEEN MARGARET:\n",
      "Ay, and me God, came, somelooks;\n",
      "And not conce, and thee, reason in was for to my heart;\n",
      "Sphap those torness!\n",
      "\n",
      "QUE:\n",
      "That Eight with the what's bestict.\n",
      "\n",
      "BOLTHASd STAN:\n",
      "Whunder I came, leave afth, but upon Wiforse cut,\n",
      "And my sputill courages your hunghts,\n",
      "And sparr beggtihard! and highAngnings?\n",
      "\n",
      "SLICINDUD:\n",
      "May is aftend Sawwarestt, and nor broke\n",
      "And the proud of clanusly paor then,\n",
      "Were thanks of them man? A laid betters.\n",
      "Where, if any the should her lam his as must\n",
      "all againt loving her kneets,\n",
      "Oared his sworn a plovought fing of barreasurar to thy sting and the kingly how be igness;\n",
      "For in leaving to rairs me too\n",
      "as that a wisescome rus as thine weeps,\n",
      "Sin the kissy the visolians, love the draw ic a news,\n",
      "Bmoring in mine kings appest more:\n",
      "Which uphat, our the worlds man;\n",
      "I kindming gere; and as alles in while vow.\n",
      "\n",
      "CORIOe?\n",
      "\n",
      "FRIAR\n",
      "BALLANNE:\n",
      "Tiz parce that that Clowess 'tis it the gain dater\n",
      "A thoing sick in the gainss, word,\n",
      "Be it flomes sworn the come in seend I with the king.\n",
      "\n",
      "LEONTES:\n",
      "O Najese the I passe:\n",
      "Mony paperity moreough hing, all and thou rodge:\n",
      "What to housd sorrow-sning behing like: proked.\n",
      "one thou do, come, retter, you got come,\n",
      "And now reast is sack hought shall to seny\n",
      "Long greath I was son\n",
      "And prover-eal beseep to my brodd;\n",
      "We me hap-mold shap plass's haid,\n",
      "And you hang you\n",
      "I pusin'd Miner hear'st necinchreas,\n",
      "I am at still passim he resing the fundel,\n",
      "Trend the dlssance insispirant, nyself my hold\n",
      "And lam my quiriefy, miny loves; you onged\n",
      "Merhart the reveen and worsed:\n",
      "For deser retchither templess to me;\n",
      "A vispose upon he behore the quide lip, mark,\n",
      "Would that as hung tose you at as cast us west stands\n",
      "I way a malife mish frievers by sofall\n",
      "Or me thind'ly ose dead; reheir come,\n",
      "Though pansward I come a sage, rong is mise\n",
      "I d\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16           # how many independent sequences will we process in parallel\n",
    "block_size = 32           # what is the maximum context length for predictions\n",
    "max_iters = 5000          # max number of iterations in training\n",
    "eval_interval = 100       # every once in a while (intervals of size 'eval_interval') evaluate the loss on 'train' and 'val' sets\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # The ability to run on a 'GPU' if we have it : use 'cuda' instead of cpu, for faster computations \n",
    "eval_iters = 200\n",
    "n_embd = 64               # embedding dimension (C)\n",
    "n_head = 4                # number of heads\n",
    "n_layer = 4               # 4 layers\n",
    "dropout = 0.0             # regularization\n",
    "# -------------------------------------------------\n",
    "\n",
    "torch.manual_seed(1989)\n",
    "\n",
    "# upload the file\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)  # if cuda is used, we move the data to device\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad() # this context manager torch.no_grad tells Pytroch that everything that happens inside the function \"estimate_loss\", we will not call .backward() on.\n",
    "def estimate_loss():\n",
    "    \"\"\"it averages up the loss over multiple batches\"\"\"\n",
    "    \n",
    "    out = {} # empty dict for losses for each iteration in the evaluation\n",
    "    model.eval() # setting the model into evaluation phase\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y) # outputs of .forward method in \"BigramLanguageModel\" class \n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # resetting the model into training phase\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__() # inhereting additional attributes from \"nn.Module\"\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout) # regularization\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\") : No communication with the future\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T) # head_size**-0.5 for scaling\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei) # regularization\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout) # regularization\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)  # concat over the channel dimension\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout), # regularization\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)   # communication\n",
    "        self.ffwd = FeedFoward(n_embd)                    # computation\n",
    "        self.ln1 = nn.LayerNorm(n_embd)                   # normalization\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))    # communication\n",
    "        x = x + self.ffwd(self.ln2(x))  # computation\n",
    "        return x\n",
    "\n",
    "# simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond) # calls the \"forward\" method\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)   # if cuda is used, it moves the model parameters to device= cude\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'Million parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    Xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(Xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device) # create on the device when creating the context that feeds into \"generate\"\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b14bd87",
   "metadata": {},
   "source": [
    "# Change Log\n",
    "\n",
    "| Date (DD-MM-YYYY) | Version | Changed By      | Change Description      |\n",
    "| ----------------- | ------- | -------------   | ----------------------- |\n",
    "| 02-04-2024       | 1.0     | Younes Dahami   |  initial version |\n",
    "|22-04-2024        | 1.1    | Younes Dahami     |adding images/ updating the code |\n",
    "|27-04-2024        | 1.2    | Younes Dahami     |adding images/ updating the code |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de18068",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
