{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5e9c216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./input.txt', <http.client.HTTPMessage at 0x2388fca72b0>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "urlretrieve(url, './input.txt') # create a text file named input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a791d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read it in to inspect it\n",
    "with open('./input.txt','r', encoding='utf-8') as f :\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee758e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115394\n"
     ]
    }
   ],
   "source": [
    "# length of the text in characters\n",
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8b80128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "Second Citizen:\n",
      "Would you proceed especially against Caius Marcius?\n",
      "\n",
      "All:\n",
      "Against him first: he's a very dog to the commonalty.\n",
      "\n",
      "Second Citizen:\n",
      "Consider you what services he has done for his country?\n",
      "\n",
      "First Citizen:\n",
      "Very well; and could be content to give him good\n",
      "report fort, but that he pays himself with being proud.\n",
      "\n",
      "Second Citizen:\n",
      "Nay, but speak not maliciously.\n",
      "\n",
      "First Citizen:\n",
      "I say unto you, what he hath done famously, he did\n",
      "it to that end: though soft-conscienced men can be\n",
      "content to \n"
     ]
    }
   ],
   "source": [
    "# Let's look at the first 1500 charcters\n",
    "print(text[:1500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7350ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "the number of unique characters in the text : 65\n"
     ]
    }
   ],
   "source": [
    "# All the unique character that occur in the text\n",
    "chars = sorted(list(set(text)))  # vocabulary\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(''.join(chars))\n",
    "print(f'the number of unique characters in the text : {vocab_size}')  # nb of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c65f73b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "\n",
    "# mapping from integers to characters\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# chracter-level tokenizer\n",
    "# encoder: takes a string, output a list of integers\n",
    "encode = lambda s: [stoi[c] for c in s] \n",
    "# decoder: take a list of integers, output a string\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) \n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb9938e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56,  ..., 58, 53,  1])\n"
     ]
    }
   ],
   "source": [
    "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype= torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "\n",
    "# the 1500 characters we looked at earlier that will go the GPT look like this\n",
    "print(data[:1500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "101beb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now split up the data into train and validation sets\n",
    "# first 90% will be train, rest val\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69efcdba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# context length  ; time dimension\n",
    "block_size = 8\n",
    "train_data[:block_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef70e8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target: 47\n",
      "when input is tensor([18, 47]) the target: 56\n",
      "when input is tensor([18, 47, 56]) the target: 57\n",
      "when input is tensor([18, 47, 56, 57]) the target: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]     # the input to the transformer\n",
    "y = train_data[1:block_size+1]  # targets for each position\n",
    "\n",
    "for t in range(block_size) :\n",
    "    # In fact, each input example has 'block_size' examples in it\n",
    "    context = x[:t+1]    # t+1 is exlusive\n",
    "    target = y[t]\n",
    "    \n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f851e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "----\n",
      "when input is [24] the target: 43\n",
      "when input is [24, 43] the target: 58\n",
      "when input is [24, 43, 58] the target: 5\n",
      "when input is [24, 43, 58, 5] the target: 57\n",
      "when input is [24, 43, 58, 5, 57] the target: 1\n",
      "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
      "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
      "when input is [44] the target: 53\n",
      "when input is [44, 53] the target: 56\n",
      "when input is [44, 53, 56] the target: 1\n",
      "when input is [44, 53, 56, 1] the target: 58\n",
      "when input is [44, 53, 56, 1, 58] the target: 46\n",
      "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
      "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
      "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52] the target: 58\n",
      "when input is [52, 58] the target: 1\n",
      "when input is [52, 58, 1] the target: 58\n",
      "when input is [52, 58, 1, 58] the target: 46\n",
      "when input is [52, 58, 1, 58, 46] the target: 39\n",
      "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
      "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
      "when input is [25] the target: 17\n",
      "when input is [25, 17] the target: 27\n",
      "when input is [25, 17, 27] the target: 10\n",
      "when input is [25, 17, 27, 10] the target: 0\n",
      "when input is [25, 17, 27, 10, 0] the target: 21\n",
      "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
      "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
      "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)  # for reproducibility\n",
    "batch_size = 4 # B : how many independent sequences will we process in parallel?\n",
    "block_size = 8 # T : what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    \"\"\"generate a small batch of data of inputs x and targets y\"\"\"\n",
    "    \n",
    "    data = train_data if split == 'train' else val_data\n",
    "    \n",
    "    # sampling random locations in the dataset to pull chunks from, between 0 and 'len(data) - block_size', shape(batch_size,0)\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    \n",
    "    # stack the rows to become (batch_size,block_size) tensors\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])     # we tack because data is a list; shape(B,T)\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # because each example has multiple examples in it; shape(B,T)\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')  # b for batch\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size): # batch dimension (row dim)\n",
    "    for t in range(block_size): # time dimension (col dim)\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc51b11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
     ]
    }
   ],
   "source": [
    "# our batch of input to the transformer\n",
    "print(xb) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "155266bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of logits after forward is applied : torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()  # inherets additional attributes from nn.Module\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) # shape(65,65)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C) : Batch, Time(block_size), Channels(vocab_size) embedding dimension\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)  # that's how pytorch's crossentropy treats the logits dim in crossentropy  \n",
    "            targets = targets.view(B*T)   # reshape the targets (B,T)----> B*T\n",
    "            loss = F.cross_entropy(logits, targets)  # loss\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \"\"\" idx is (B, T) array of indices in the current context in some batch.\n",
    "        The job of generate is basically to take (B,T) and extend it to be (B,T+1), (B,T+2)....\n",
    "        it basically continues the generation in all the Batch dimensions in the Time dimension. It will do\n",
    "        that for  max_new_tokens\"\"\"\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)  # applies the forward method\n",
    "            # focus only on the last time step \n",
    "            logits = logits[:, -1, :] # (B,T,C) becomes (B, C)\n",
    "            # apply softmax on the last dimension (hence -1) to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C), remember that C here is vocab_size\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1) because from each row we sample 1\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "# Calculate the loss before training    \n",
    "m = BigramLanguageModel(vocab_size)   # __init__ is applied : an embedding table (65,65) is created\n",
    "logits, loss = m(xb, yb)              # forward is applied\n",
    "print(f\"Shape of logits after forward is applied : {logits.shape}\")\n",
    "print(loss)\n",
    "\n",
    "\n",
    "# Generate 100 tokens without training\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a2c75e",
   "metadata": {},
   "source": [
    "`idx = torch.zeros((1, 1),dtype=torch.long)` creates a torch tensor of shape(1,1) of type integers, with 0 in it, to kick off the generation. `[0]` to unplack the single batch (because we worked with 1 batch) dimension that exists, and we convert it to a list to feed it to `decode`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d972b70",
   "metadata": {},
   "source": [
    "## Training the bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa3018be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0319fdbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.721843719482422\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(1000): # increase number of steps for good results...\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    \n",
    "    # zeroing out all the gradients from the previous step\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    # getting the gradients for all the parameters\n",
    "    loss.backward()\n",
    "    \n",
    "    # using those gradients to update our prameters\n",
    "    optimizer.step()\n",
    "\n",
    "# Print the final loss\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd932cbc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "olylvLLko'TMyatyIoconxad.?-tNSqYPsx&bF.oiR;BD$dZBMZv'K f bRSmIKptRPly:AUC&$zLK,qUEy&Ay;ZxjKVhmrdagC-bTop-QJe.H?x\n",
      "JGF&pwst-P sti.hlEsu;w:w a BG:tLhMk,epdhlay'sVzLq--ERwXUzDnq-bn czXxxI&V&Pynnl,s,Ioto!uvixwC-IJXElrgm C-.bcoCPJ\n",
      "IMphsevhO AL!-K:AIkpre,\n",
      "rPHEJUzV;P?uN3b?ohoRiBUENoV3B&jumNL;Aik,\n",
      "xf -IEKROn JSyYWW?n 'ay;:weO'AqVzPyoiBL? seAX3Dot,iy.xyIcf r!!ul-Koi:x pZrAQly'v'a;vEzN\n",
      "BwowKo'MBqF$PPFb\n",
      "CjYX3beT,lZ qdda!wfgmJP\n",
      "DUfNXmnQU mvcv?nlnQF$JUAAywNocd  bGSPyAlprNeQnq-GRSVUP.Ja!IBoDqfI&xJM AXEHV&DKvRS\n"
     ]
    }
   ],
   "source": [
    "# Let's generate 500 tokens after this first training\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c76570",
   "metadata": {},
   "source": [
    "## The mathematical trick in self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7118e751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# consider the following toy example:\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2 # batch, time (context), channels (embedding_size)\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6220a5db",
   "metadata": {},
   "source": [
    "These 8 tokens are not talking, to each other, and we want them to talk to each other, in such a way that token $t$ only communicates with with previous tokens and itself, and it should not communicate with tokens after it, because those are future tokens in the sequence, **no information come from the future because we're about to predict the future.** (masked attention for decoder-only transformer)\n",
    "\n",
    "The easiest way for tokens to communicate is calculating the average. So for example, if I'm the 5th token $t_5$, I would like to take the channels that are information at my step (the 5th), but also the channels from the 4th step, 3rd...1st step ($t_4$, $t_3$, $t_2$, $t_1$), and I would like to average those up, and that would become sort of like a **feature vector that summarizes me in the context of my history.** \n",
    "\n",
    "Calculating the average is actually extremely weak and lossy form of interaction, where we lose a ton of information about the spatial arrangement of all those tokens, but that's ok for now, we'll see how to bring that information back later.\n",
    "\n",
    "For now what we would like to do is : For every single batch element independently, for every t-th token in that sequence, we'd like to calculate the average of all the vectors in all the previous tokens and the current token ($i\\leq t$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "829ea2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want x[b,t] = mean_{i<=t} x[b,i] \n",
    "# bow : bag of words\n",
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # shape(t,C)\n",
    "        xbow[b,t] = torch.mean(xprev, 0)   # we average on the t (dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "804932f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.3596, -0.9152],\n",
       "        [ 0.6258,  0.0255],\n",
       "        [ 0.9545,  0.0643],\n",
       "        [ 0.3612,  1.1679],\n",
       "        [-1.3499, -0.5102],\n",
       "        [ 0.2360, -0.2398],\n",
       "        [-0.9211,  1.5433]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first element of the batch \n",
    "x[0]  #(T,C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10478651",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0] #averging the x[0] on the t (T,C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa413d46",
   "metadata": {},
   "source": [
    "Each i-th row from `xbow[0]` is the average of the 0,1,...,i-th rows from `x[0]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b159907",
   "metadata": {},
   "source": [
    "## Calculating the average using matrix multiplication for a weighted aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6fa3cb98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# triangular lower portion\n",
    "torch.tril(torch.ones(3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04ccac49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# these 2 lines of a, help us calculate the averages \n",
    "a = torch.tril(torch.ones(3, 3))    # triangular lower portion\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f2c6f7",
   "metadata": {},
   "source": [
    "We use the tringular matrix so that each i-th row from `c`, is the average of rows 0,1,...i-th from `b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06f8bc9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril(torch.ones(T, T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40185d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 2: using matrix multiply for a weighted aggregation\n",
    "# wei short for weights\n",
    "\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)  # for averaging\n",
    "xbow2 = wei @ x # (T, T) @ (B, T, C)-----Pytorch----->(B, T, T) @ (B, T, C) ----> (B, T, C)\n",
    "#torch.allclose(xbow, xbow2)  # True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e117f8e",
   "metadata": {},
   "source": [
    "**Explanation of (B, T, T) @ (B, T, C) ----> (B, T, C) :**\n",
    "\n",
    "In reality, it's **(T, T) @ (B, T, C)**, but Pytorch will come and see that these shapes are not the same, so it will create a batch dimension in (T, T), the mulitplication become **(B, T, T) @ (B, T, C)**. And `@` is a **batched matrix multiply**, and so it will apply this matrix multiplication in all the batch elements in **parallel** and **individually**. And then for each batch element there will be a **(T, T) @ (T, C)**, exaclty what we had above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040a0c4d",
   "metadata": {},
   "source": [
    "## Using Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e5c254e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 8])\n",
      "torch.Size([4, 8, 2])\n",
      "torch.Size([4, 8, 2])\n"
     ]
    }
   ],
   "source": [
    "# version 3: use Softmax\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T,T)) #(T,T)\n",
    "# all the tril elements which are equal to zero make them equal to -inf\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) #(T,T)\n",
    "# exponentiate every element of wei and divide by their sum; the future cannot communicate with the past\n",
    "wei = F.softmax(wei, dim=-1)  # dim =-1 refers to the last dimension of wei #(T,T)\n",
    "print(wei.shape)\n",
    "print(x.shape)\n",
    "xbow3 = wei @ x  # (T,T)@(B,T,C)--->(B,T,T)@(B,T,C)--->(B,T,C)\n",
    "print(xbow3.shape)\n",
    "#torch.allclose(xbow, xbow3)  # True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d2149c",
   "metadata": {},
   "source": [
    "## Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1fe92b7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 4: self-attention!\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, time, channels (we one batch of 4 examples, each example is composed of T tokens, each tokens is represented in C-dim embedding)\n",
    "x = torch.randn(B,T,C)  #(B,T,C)\n",
    "\n",
    "# let's see a single Head perform self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False) # dim_input=C, and dim_output=head_size\n",
    "query = nn.Linear(C, head_size, bias=False) # bias=False means where just calculating a matrix multiplication\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "# all the tokens in all the positions in (B,T) arrangements will produce (in parallel and independetly) a key and a query,\n",
    "# no communication has happened yet\n",
    "k = key(x)   # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "\n",
    "# communication happens now, we transpose k in the 2nd and 3rd dimensions\n",
    "# we're applying T so that the dimensiosn of q and k allign\n",
    "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "\n",
    "# to avoid the communication with the future : Masked self-attention\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "#wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "\n",
    "# Calculate similiraty scores (weights)\n",
    "wei = F.softmax(wei, dim=-1)  #(T,T)\n",
    "\n",
    "# Calculate attention scores\n",
    "v = value(x)   # (B, T, 16)\n",
    "out = wei @ v  # (T,T)@(B, T, 16)-->(B,T,T)@(B,T,16)------->(B,T,16)\n",
    "#out = wei @ x\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "07ae30c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Represents the similarity scores in the first example in the batch\n",
    "wei[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5de2b76",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- Attention is a **communication mechanism**, it can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "\n",
    "- There is no notion of space. Attention simply acts over a set of vectors. **This is why we need to positionally encode tokens.**\n",
    "\n",
    "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "\n",
    "- In an **encoder attention block**, we just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a **decoder attention block** because it has **triangular masking**, and is usually used in autoregressive settings, like language modeling.\n",
    "\n",
    "- **Self-attention** just means that the keys and values are produced from the same source as queries. In **cross-attention**, the queries still get produced from `x`, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "\n",
    "- **Scaled attention additional** divides `wei` by $1/\\sqrt(HeadSize)$. This makes it so when input Q,K are unit variance ($Var(Q)=Var(K)=1$), `wei` will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3a66cda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.randn(B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1d01494b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.0449), tensor(1.0700), tensor(1.0918))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.var(), q.var(), wei.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "43f56239",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e8a1062c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])\n",
      "tensor([1.5851e-02, 7.8918e-04, 1.1713e-01, 7.8918e-04, 8.6545e-01])\n",
      "tensor([2.3555e-03, 2.6167e-05, 4.7312e-02, 2.6167e-05, 9.5028e-01])\n",
      "tensor([4.2484e-18, 3.9754e-31, 2.0612e-09, 3.9754e-31, 1.0000e+00])\n"
     ]
    }
   ],
   "source": [
    "print(torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1))\n",
    "print(torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*10, dim=-1))\n",
    "print(torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*15, dim=-1))\n",
    "print(torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*100, dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b74f19",
   "metadata": {},
   "source": [
    "**Remark :** When the variance diverges from 1 the softmax gets too peaky, converges to one-hot as seen in the example above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42fb03e",
   "metadata": {},
   "source": [
    "## Normalization \n",
    "\n",
    "The normalization layer normalize the output of the attention mechanisms to stabilize training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2ac8184f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LayerNorm1d:\n",
    "    # (used to be BatchNorm1d)  # identical to layernorm in pytorch\n",
    "    # Nomalizing rows instead of columns (like in the \"BatchNorm1d\" class in part 3 from the makemore code (part 3))\n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1):  \n",
    "        self.eps = eps  # epsilon term in the normalization\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # calculate the forward pass\n",
    "        xmean = x.mean(1, keepdim=True) # batch mean ; shape(x)\n",
    "        xvar = x.var(1, keepdim=True) # batch variance ; shape(x)\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance (B,T,C)\n",
    "        self.out = self.gamma * xhat + self.beta # same shape(x)\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]  # trainable parameters\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "module = LayerNorm1d(100)\n",
    "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
    "x = module(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "de2f1333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1469), tensor(0.8803))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs (not normalized) (1st column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "59a2be43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-9.5367e-09), tensor(1.0000))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features (normalized) (1st row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "39b33c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# French to English translation example:\n",
    "# Encoder-decoder tranformer\n",
    "# <--------- ENCODE ------------------><--------------- DECODE ----------------->\n",
    "# les r√©seaux de neurones sont fantastiques! <START> neural networks are fantastic!<END>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7887e32",
   "metadata": {},
   "source": [
    "## Full finished code, for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3e98df88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.209729 Million parameters\n",
      "step 0: train loss 4.4116, val loss 4.4022\n",
      "step 100: train loss 2.6568, val loss 2.6670\n",
      "step 200: train loss 2.5091, val loss 2.5059\n",
      "step 300: train loss 2.4193, val loss 2.4334\n",
      "step 400: train loss 2.3502, val loss 2.3562\n",
      "step 500: train loss 2.2967, val loss 2.3132\n",
      "step 600: train loss 2.2408, val loss 2.2499\n",
      "step 700: train loss 2.2054, val loss 2.2188\n",
      "step 800: train loss 2.1638, val loss 2.1869\n",
      "step 900: train loss 2.1246, val loss 2.1511\n",
      "step 1000: train loss 2.1035, val loss 2.1311\n",
      "step 1100: train loss 2.0704, val loss 2.1190\n",
      "step 1200: train loss 2.0383, val loss 2.0801\n",
      "step 1300: train loss 2.0257, val loss 2.0651\n",
      "step 1400: train loss 1.9924, val loss 2.0374\n",
      "step 1500: train loss 1.9694, val loss 2.0288\n",
      "step 1600: train loss 1.9632, val loss 2.0486\n",
      "step 1700: train loss 1.9408, val loss 2.0135\n",
      "step 1800: train loss 1.9085, val loss 1.9942\n",
      "step 1900: train loss 1.9096, val loss 1.9889\n",
      "step 2000: train loss 1.8855, val loss 1.9945\n",
      "step 2100: train loss 1.8708, val loss 1.9760\n",
      "step 2200: train loss 1.8592, val loss 1.9594\n",
      "step 2300: train loss 1.8544, val loss 1.9508\n",
      "step 2400: train loss 1.8416, val loss 1.9416\n",
      "step 2500: train loss 1.8182, val loss 1.9454\n",
      "step 2600: train loss 1.8290, val loss 1.9396\n",
      "step 2700: train loss 1.8110, val loss 1.9323\n",
      "step 2800: train loss 1.8057, val loss 1.9257\n",
      "step 2900: train loss 1.8018, val loss 1.9313\n",
      "step 3000: train loss 1.7970, val loss 1.9216\n",
      "step 3100: train loss 1.7673, val loss 1.9171\n",
      "step 3200: train loss 1.7545, val loss 1.9118\n",
      "step 3300: train loss 1.7590, val loss 1.9131\n",
      "step 3400: train loss 1.7543, val loss 1.8974\n",
      "step 3500: train loss 1.7376, val loss 1.8942\n",
      "step 3600: train loss 1.7258, val loss 1.8898\n",
      "step 3700: train loss 1.7278, val loss 1.8833\n",
      "step 3800: train loss 1.7184, val loss 1.8933\n",
      "step 3900: train loss 1.7232, val loss 1.8772\n",
      "step 4000: train loss 1.7152, val loss 1.8644\n",
      "step 4100: train loss 1.7152, val loss 1.8797\n",
      "step 4200: train loss 1.7074, val loss 1.8707\n",
      "step 4300: train loss 1.7013, val loss 1.8505\n",
      "step 4400: train loss 1.7048, val loss 1.8687\n",
      "step 4500: train loss 1.6885, val loss 1.8489\n",
      "step 4600: train loss 1.6888, val loss 1.8361\n",
      "step 4700: train loss 1.6827, val loss 1.8452\n",
      "step 4800: train loss 1.6663, val loss 1.8457\n",
      "step 4900: train loss 1.6689, val loss 1.8397\n",
      "step 4999: train loss 1.6627, val loss 1.8243\n",
      "\n",
      "ROMEO:\n",
      "But you freither'd wish his migute:\n",
      "No, dour I uspracely as too over God's grance that must of touch\n",
      "The wrutumpution alond you glifand.\n",
      "Tile eybernings to her of the found Husaming,\n",
      "His by blow, if thy bew, gub, if norith his ghards wifith of Homh nother, sI a hiswords non.\n",
      "\n",
      "HES MORTENCUS:\n",
      "Hy tamber, cleens us\n",
      "By thou sovore on this withip art sade but grove to him, he's subdorent you,\n",
      "And maithe your toutherly nather.\n",
      "\n",
      "NORTHUMBERDIZABETH:\n",
      "Thou sir, what in the sceed with mad his god and enirsely moy bedsly comfals!\n",
      "What cumforself this grain. Was that be us.\n",
      "\n",
      "LUCENTIA:\n",
      "Has lew, ever some,:\n",
      "Fantwer and heart I sly-boutt him\n",
      "is fortunuel marry whose my later,\n",
      "Stay thy subst fall us belies! Lord:\n",
      "We a-mone Hering like, for with!\n",
      "He a Recrompechrive Affrorceling,\n",
      "And confest Warwizely unward stroptrightrey,\n",
      "And with all us I'll bear confetio-s.\n",
      "\n",
      "HENRY VI:\n",
      "O.\n",
      "\n",
      "BRUCKINGHASS:\n",
      "O, you, are with me.\n",
      "\n",
      "HASFird's there?\n",
      "\n",
      "MORGARY:\n",
      "O name fornumasts.\n",
      "\n",
      "MENEN VINCENTIO:\n",
      "When my your vow; being whill the.\n",
      "Is that was but to good it\n",
      "Lone will thy dukes, I wark than wark you, banot thus orly what for vilest\n",
      "How duth with you and well stopproad!\n",
      "Lay, brauty they worse is senst my brunow med like of suppramestle;\n",
      "And of the light mark own heave it.\n",
      "\n",
      "Secontriving thesoo, in thus my nome trathing to suporale\n",
      "On think, if by yold, thou how or nother,\n",
      "I'll Gonged these deat swear\n",
      "in That ant: In I\n",
      "surn of shall! I play'st I anders?\n",
      "And the nuear will the curse, I're;\n",
      "Will prown all, to am you say?\n",
      "\n",
      "POMPEY:\n",
      "Yow grave a mostsmer all nob, but us,\n",
      "Tattents as wattelens not of yoorren;\n",
      "That pee nonet strefore's too nead?\n",
      "\n",
      "LORWISSUCELIZABELLA:\n",
      "A gray thray mover a deseried with.\n",
      "Your hurshmont,\n",
      "First well beforeius, sweets suil his nor trmyselful I.\n",
      "\n",
      "JOH.\n",
      "\n",
      "MENd Clivere be pare feek, heart where own thee, with you,\n",
      "My leave is is in mee tothy, be prostless that high prauth old know,\n",
      "Thonstrume make Dicking my vaintingn shall consure,\n",
      "Which prant who' been? bigh you not life,\n",
      "Than I know to\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000 # max number of iterations in training\n",
    "eval_interval = 100  # every once in a while (intervals of size eval_interval) evaluate the loss on train and val sets\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # The ability to run on a GPU if you have it : use cuda instead of cpu, for faster computations \n",
    "eval_iters = 200\n",
    "n_embd = 64   # embedding dimension (C)\n",
    "n_head = 4\n",
    "n_layer = 4   # 4 layers\n",
    "dropout = 0.0 # regularization\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)  # if cuda is used, we move the data to device\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad() # this context manager torch.no_grad tells Pytroch that everything that happens inside the function \"estimate_loss\", we will not call .backward() on.\n",
    "def estimate_loss():\n",
    "    \"\"\"it averages up the loss over multiple batches\"\"\"\n",
    "    \n",
    "    out = {} # empty dict for losses for each iteration in the evaluation\n",
    "    model.eval() # setting the model into evaluation phase\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y) # outputs of .forward method in \"BigramLanguageModel\" class \n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # resetting the model into training phase\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__() # inhereting additional attributes from \"nn.Module\"\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout) # regularization\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\") : No communication with the future\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T) # head_size**-0.5 for scaling\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei) # regularization\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout) # regularization\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)  # concat over the channel dimension\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout), # regularization\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)   # communication\n",
    "        self.ffwd = FeedFoward(n_embd)                    # computation\n",
    "        self.ln1 = nn.LayerNorm(n_embd)                   # normalization\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))    # communication\n",
    "        x = x + self.ffwd(self.ln2(x))  # computation\n",
    "        return x\n",
    "\n",
    "# simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond) # calls the \"forward\" method\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)   # if cuda is used, it moves the model parameters to device= cude\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'Million parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device) # create on the device when creating the context that feeds into \"generate\"\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa5da18",
   "metadata": {},
   "source": [
    "# Remark : Why do we sample from a multinomial distribution  ?\n",
    "\n",
    "Using the maximum probability token at each step during generation, also known as **greedy decoding**, can lead to suboptimal results in certain scenarios. Here's why sampling from the multinomial distribution is preferred over greedy decoding:\n",
    "\n",
    "* **Diversity:** Greedy decoding tends to produce repetitive or deterministic outputs because it always selects the token with the highest probability. This can result in generated sequences lacking diversity and variety. Sampling from the multinomial distribution allows for randomness in token selection, introducing diversity in the generated outputs.\n",
    "\n",
    "* **Exploration:** Sampling encourages the model to explore different possibilities in the output space. By considering tokens with lower probabilities, the model has the opportunity to generate alternative sequences that may not be immediately apparent based on the highest probability tokens alone. This exploration can lead to more creative and diverse outputs.\n",
    "\n",
    "* **Handling Uncertainty:** In some cases, the model may have high uncertainty about the next token to generate, with multiple tokens having relatively high probabilities. Sampling allows the model to express this uncertainty by considering multiple tokens with non-negligible probabilities, rather than committing to a single token deterministically.\n",
    "\n",
    "* **Avoiding Mode Collapse:** Greedy decoding is susceptible to mode collapse, where the model repeatedly generates similar or identical sequences. Sampling helps mitigate this issue by introducing randomness into the generation process, preventing the model from getting stuck in repetitive patterns.\n",
    "\n",
    "Overall, sampling from the multinomial distribution provides a more flexible and exploratory approach to sequence generation, allowing the model to produce diverse and varied outputs while handling uncertainty and avoiding mode collapse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e48a6a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
