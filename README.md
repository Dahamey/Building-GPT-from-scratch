In this project, we constructed a Generatively Pretrained Transformer (GPT), a Decoder-Only Transformer, inspired by the paper 'Attention is All You Need' and OpenAI's GPT-2 / GPT-3 models.
